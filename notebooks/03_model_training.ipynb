"""
# Model Training and Evaluation

## Training and comparing different models for medical image analysis
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import sys

sys.path.append('../src')
from models.lightweight_cnn import LightweightMedicalCNN
from models.attention_network import AttentionMedicalCNN
from models.traditional_ml import TraditionalMLModels
from trainer.model_trainer import MedicalModelTrainer
from evaluator.visualization import ResultVisualizer

def create_data_generators(data_path, batch_size=32, image_size=(224, 224)):
    """Create data generators for training and validation"""
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        zoom_range=0.2,
        validation_split=0.2
    )
    
    train_generator = train_datagen.flow_from_directory(
        data_path,
        target_size=image_size,
        batch_size=batch_size,
        class_mode='binary',
        subset='training'
    )
    
    val_generator = train_datagen.flow_from_directory(
        data_path,
        target_size=image_size,
        batch_size=batch_size,
        class_mode='binary',
        subset='validation'
    )
    
    return train_generator, val_generator

def train_cnn_model(model, train_data, val_data, model_name):
    """Train CNN model"""
    trainer = MedicalModelTrainer(model, model_name)
    trainer.compile_model(learning_rate=0.001)
    
    history = trainer.train(train_data, val_data, epochs=50)
    results = trainer.evaluate(val_data)
    
    return history, results

def evaluate_model_performance(model, test_data, model_name):
    """Comprehensive model evaluation"""
    # Predictions
    y_pred = model.predict(test_data)
    y_true = test_data.classes
    
    # Convert probabilities to class predictions
    y_pred_classes = np.argmax(y_pred, axis=1)
    
    # Classification report
    report = classification_report(y_true, y_pred_classes, output_dict=True)
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred_classes)
    
    # ROC curve
    fpr, tpr, _ = roc_curve(y_true, y_pred[:, 1])
    roc_auc = auc(fpr, tpr)
    
    return {
        'classification_report': report,
        'confusion_matrix': cm,
        'roc_curve': (fpr, tpr, roc_auc),
        'predictions': y_pred,
        'true_labels': y_true
    }

def compare_models(results_dict):
    """Compare performance of different models"""
    comparison_data = []
    
    for model_name, results in results_dict.items():
        report = results['classification_report']
        comparison_data.append({
            'model': model_name,
            'accuracy': report['accuracy'],
            'precision': report['weighted avg']['precision'],
            'recall': report['weighted avg']['recall'],
            'f1_score': report['weighted avg']['f1-score'],
            'auc': results['roc_curve'][2]
        })
    
    return pd.DataFrame(comparison_data)

# Main training workflow
if __name__ == "__main__":
    DATA_PATH = "../data/processed"
    
    # Create data generators
    train_gen, val_gen = create_data_generators(DATA_PATH)
    
    # Initialize models
    models = {
        'lightweight_cnn': LightweightMedicalCNN(num_classes=2),
        'attention_cnn': AttentionMedicalCNN(num_classes=2)
    }
    
    # Train models
    results_dict = {}
    for model_name, model in models.items():
        print(f"Training {model_name}...")
        history, results = train_cnn_model(model, train_gen, val_gen, model_name)
        results_dict[model_name] = results
        
        # Plot training history
        visualizer = ResultVisualizer()
        visualizer.plot_training_history(
            history, 
            f"../output/visualizations/{model_name}_training.png"
        )
    
    # Compare models
    comparison_df = compare_models(results_dict)
    print("\nModel Comparison:")
    print(comparison_df)
    
    # Visualize comparison
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']
    
    for i, metric in enumerate(metrics):
        ax = axes[i // 3, i % 3]
        comparison_df.plot.bar(x='model', y=metric, ax=ax, legend=False)
        ax.set_title(f'{metric.replace("_", " ").title()}')
        ax.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('../output/visualizations/model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
